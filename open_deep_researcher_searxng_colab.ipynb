{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomsenDrake/SearXNG-Researcher/blob/main/open_deep_researcher_searxng_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7cTpP9rDZW-"
      },
      "outputs": [],
      "source": [
        "!pip install nest_asyncio python-dotenv bs4 aiohttp requests\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJTo96a7DGUz"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "from typing import Optional\n",
        "from datetime import datetime, timedelta\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration Constants\n",
        "# =======================\n",
        "GEMINI_API_KEY = \"YOUR_GEMINI_API_KEY\"  # Replace with your Gemini API key\n",
        "\n",
        "# Endpoints\n",
        "GEMINI_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\"\n",
        "SEARXNG_URL = \"LOCALHOST:8080\"  # Replace with your SearXNG URL\n",
        "\n",
        "# Default LLM model\n",
        "DEFAULT_MODEL = \"gemini-2.0-flash\"\n",
        "\n",
        "# Add rate limiting constants\n",
        "RATE_LIMIT_CALLS = 15  # Maximum calls per minute\n",
        "RATE_LIMIT_PERIOD = 60  # Period in seconds\n",
        "RATE_LIMIT_DELAY = RATE_LIMIT_PERIOD / RATE_LIMIT_CALLS  # ~4 seconds between calls\n",
        "MAX_RETRIES = 3  # Maximum number of retries for rate-limited requests\n",
        "\n",
        "# Rate limiting queue\n",
        "call_timestamps = deque(maxlen=RATE_LIMIT_CALLS)\n",
        "\n",
        "async def wait_for_rate_limit():\n",
        "    \"\"\"\n",
        "    Ensure we don't exceed rate limits by tracking API calls and waiting when necessary.\n",
        "    \"\"\"\n",
        "    now = datetime.now()\n",
        "\n",
        "    # Remove timestamps older than our rate limit period\n",
        "    while call_timestamps and (now - call_timestamps[0]) > timedelta(seconds=RATE_LIMIT_PERIOD):\n",
        "        call_timestamps.popleft()\n",
        "\n",
        "    # If we've made maximum calls within the period, wait until we can make another\n",
        "    if len(call_timestamps) >= RATE_LIMIT_CALLS:\n",
        "        wait_time = (call_timestamps[0] + timedelta(seconds=RATE_LIMIT_PERIOD) - now).total_seconds()\n",
        "        if wait_time > 0:\n",
        "            print(f\"Rate limit approaching, waiting {wait_time:.1f}s\")\n",
        "            await asyncio.sleep(wait_time)\n",
        "\n",
        "    # Add current timestamp to our queue\n",
        "    call_timestamps.append(now)\n",
        "\n",
        "async def call_openrouter_async(session, messages, model=DEFAULT_MODEL, retry_count=0) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Asynchronously call the Gemini chat completion API with rate limiting and retries.\n",
        "    Returns the content of the assistant's reply or None on failure.\n",
        "    \"\"\"\n",
        "    if retry_count >= MAX_RETRIES:\n",
        "        print(\"Max retries reached for API call\")\n",
        "        return None\n",
        "\n",
        "    # Wait for rate limit before making call\n",
        "    await wait_for_rate_limit()\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {GEMINI_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        async with session.post(GEMINI_URL, headers=headers, json=payload) as resp:\n",
        "            if resp.status == 200:\n",
        "                result = await resp.json()\n",
        "                try:\n",
        "                    return result['choices'][0]['message']['content']\n",
        "                except (KeyError, IndexError) as e:\n",
        "                    print(\"Unexpected Gemini response structure:\", result)\n",
        "                    return None\n",
        "            elif resp.status == 429:\n",
        "                # Rate limit hit - wait longer and retry\n",
        "                wait_time = (retry_count + 1) * RATE_LIMIT_DELAY * 2\n",
        "                print(f\"Rate limit hit, waiting {wait_time}s before retry {retry_count + 1}/{MAX_RETRIES}\")\n",
        "                await asyncio.sleep(wait_time)\n",
        "                return await call_openrouter_async(session, messages, model, retry_count + 1)\n",
        "            else:\n",
        "                text = await resp.text()\n",
        "                print(f\"Gemini API error: {resp.status} - {text}\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(\"Error calling Gemini:\", e)\n",
        "        return None\n",
        "\n",
        "async def generate_search_queries_async(session, user_query):\n",
        "    \"\"\"\n",
        "    Ask the LLM to produce up to four precise search queries (in Python list format)\n",
        "    based on the userâ€™s query.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"You are an expert research assistant. Given the user's query, generate up to four distinct, \"\n",
        "        \"precise search queries that would help gather comprehensive information on the topic. \"\n",
        "        \"Return only a Python list of strings without newlines or formatting, for example: ['query1', 'query2', 'query3'].\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and precise research assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    if response:\n",
        "        try:\n",
        "            # Clean up the response: remove code blocks, newlines, and extra whitespace\n",
        "            cleaned_response = response.replace('```python', '').replace('```', '').strip()\n",
        "            # Remove any newlines and normalize whitespace\n",
        "            cleaned_response = ' '.join(cleaned_response.split())\n",
        "            search_queries = eval(cleaned_response)\n",
        "            if isinstance(search_queries, list):\n",
        "                return search_queries\n",
        "            else:\n",
        "                print(\"LLM did not return a list. Response:\", response)\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(\"Error parsing search queries:\", e, \"\\nResponse:\", response)\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "async def perform_search_async(session, query):\n",
        "    \"\"\"\n",
        "    Asynchronously perform a search using SearXNG for the given query.\n",
        "    Returns a list of result URLs.\n",
        "    \"\"\"\n",
        "    # Note: you could add other parameters as needed.\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"engines\" : \"!web\",\n",
        "        \"format\": \"json\",\n",
        "        \"language\": \"en\",\n",
        "        \"safesearch\": 0,\n",
        "        \"pageno\": 1\n",
        "    }\n",
        "    try:\n",
        "        search_url = SEARXNG_URL.rstrip('/') + '/search'\n",
        "        print(f\"Searching with URL: {search_url}\")\n",
        "        print(f\"Search query: {query}\")\n",
        "        print(f\"Full request URL: {search_url}?{'&'.join(f'{k}={v}' for k,v in params.items())}\")  # Debug full URL\n",
        "\n",
        "        async with session.get(search_url, params=params) as resp:\n",
        "            if resp.status == 200:\n",
        "                try:\n",
        "                    results = await resp.json()\n",
        "                    print(f\"Response status: {resp.status}\")\n",
        "                    if \"results\" in results:\n",
        "                        links = [item.get(\"url\") for item in results[\"results\"] if \"url\" in item]\n",
        "                        print(f\"Found {len(links)} results\")\n",
        "                        return links\n",
        "                    else:\n",
        "                        print(\"Response content:\", results)\n",
        "                        return []\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"JSON decode error: {e}\")\n",
        "                    text = await resp.text()\n",
        "                    print(f\"Raw response: {text[:500]}\")\n",
        "                    return []\n",
        "            else:\n",
        "                text = await resp.text()\n",
        "                print(f\"SearXNG error {resp.status}: {text}\")\n",
        "                print(f\"Request URL: {str(resp.url)}\")\n",
        "                return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error performing SearXNG search: {str(e)}\")\n",
        "        print(f\"Request URL: {search_url}\")\n",
        "        return []\n",
        "\n",
        "async def fetch_webpage_text_async(session, url):\n",
        "    \"\"\"\n",
        "    Asynchronously retrieve and parse the text content of a webpage using BeautifulSoup.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Using requests instead of aiohttp for simplicity with BeautifulSoup\n",
        "        # In a production environment, you might want to use aiohttp with BS4\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse with BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.decompose()\n",
        "\n",
        "        # Get text and clean it up\n",
        "        text = soup.get_text()\n",
        "\n",
        "        # Clean up whitespace\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching webpage text: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "async def is_page_useful_async(session, user_query, page_text):\n",
        "    \"\"\"\n",
        "    Ask the LLM if the provided webpage content is useful for answering the user's query.\n",
        "    The LLM must reply with exactly \"Yes\" or \"No\".\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"You are a critical research evaluator. Given the user's query and the content of a webpage, \"\n",
        "        \"determine if the webpage contains information relevant and useful for addressing the query. \"\n",
        "        \"Respond with exactly one word: 'Yes' if the page is useful, or 'No' if it is not. Do not include any extra text.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a strict and concise evaluator of research relevance.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\nWebpage Content (first 20000 characters):\\n{page_text[:20000]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    if response:\n",
        "        answer = response.strip()\n",
        "        if answer in [\"Yes\", \"No\"]:\n",
        "            return answer\n",
        "        else:\n",
        "            # Fallback: try to extract Yes/No from the response.\n",
        "            if \"Yes\" in answer:\n",
        "                return \"Yes\"\n",
        "            elif \"No\" in answer:\n",
        "                return \"No\"\n",
        "    return \"No\"\n",
        "\n",
        "async def extract_relevant_context_async(session, user_query, search_query, page_text):\n",
        "    \"\"\"\n",
        "    Given the original query, the search query used, and the page content,\n",
        "    have the LLM extract all information relevant for answering the query.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"You are an expert information extractor. Given the user's query, the search query that led to this page, \"\n",
        "        \"and the webpage content, extract all pieces of information that are relevant to answering the user's query. \"\n",
        "        \"Return only the relevant context as plain text without commentary.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert in extracting and summarizing relevant information.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nSearch Query: {search_query}\\n\\nWebpage Content (first 20000 characters):\\n{page_text[:20000]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    if response:\n",
        "        return response.strip()\n",
        "    return \"\"\n",
        "\n",
        "async def get_new_search_queries_async(session, user_query, previous_search_queries, all_contexts):\n",
        "    \"\"\"\n",
        "    Based on the original query, the previously used search queries, and all the extracted contexts,\n",
        "    ask the LLM whether additional search queries are needed. If yes, return a Python list of up to four queries;\n",
        "    if the LLM thinks research is complete, it should return \"<done>\".\n",
        "    \"\"\"\n",
        "    context_combined = \"\\n\".join(all_contexts)\n",
        "    prompt = (\n",
        "        \"You are an analytical research assistant. Based on the original query, the search queries performed so far, \"\n",
        "        \"and the extracted contexts from webpages, determine if further research is needed. \"\n",
        "        \"If further research is needed, provide up to four new search queries as a Python list (for example, \"\n",
        "        \"['new query1', 'new query2']). If you believe no further research is needed, respond with exactly <done>.\"\n",
        "        \"\\nOutput only a Python list or the token <done> without any additional text.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a systematic research planner.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nPrevious Search Queries: {previous_search_queries}\\n\\nExtracted Relevant Contexts:\\n{context_combined}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    if response:\n",
        "        cleaned = response.strip()\n",
        "        if cleaned == \"<done>\":\n",
        "            return \"<done>\"\n",
        "        try:\n",
        "            new_queries = eval(cleaned)\n",
        "            if isinstance(new_queries, list):\n",
        "                return new_queries\n",
        "            else:\n",
        "                print(\"LLM did not return a list for new search queries. Response:\", response)\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(\"Error parsing new search queries:\", e, \"\\nResponse:\", response)\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "async def generate_report_title(session, user_query, all_contexts):\n",
        "    \"\"\"\n",
        "    Generate a concise, descriptive title for the report based on the query and findings.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Based on the user's query and the research findings, generate a concise but descriptive title \"\n",
        "        \"for the report. The title should be clear and professional, no more than 10 words. \"\n",
        "        \"Return only the title text without quotes or formatting.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a professional title writer.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Query: {user_query}\\n\\nFindings Summary:\\n{all_contexts[:1000]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    title = await call_openrouter_async(session, messages)\n",
        "    return title.strip() if title else \"Research Report\"\n",
        "\n",
        "def save_report_as_markdown(title, report_content, user_query):\n",
        "    \"\"\"\n",
        "    Save the report as a markdown file in the reports directory.\n",
        "    Returns the path to the saved file.\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "    import re\n",
        "    from datetime import datetime\n",
        "\n",
        "    notebook_dir = Path.cwd()  # Use current working directory\n",
        "    reports_dir = notebook_dir / \"reports\"\n",
        "    reports_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Create a sanitized filename from the title\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    sanitized_title = re.sub(r'[^\\w\\s-]', '', title)\n",
        "    sanitized_title = re.sub(r'[-\\s]+', '-', sanitized_title).strip('-')\n",
        "    filename = f\"{timestamp}_{sanitized_title[:50]}.md\"\n",
        "\n",
        "    filepath = reports_dir / filename\n",
        "\n",
        "    # Create the markdown content\n",
        "    markdown_content = f\"\"\"# {title}\n",
        "\n",
        "## Original Query\n",
        "{user_query}\n",
        "\n",
        "## Research Findings\n",
        "{report_content}\n",
        "\"\"\"\n",
        "\n",
        "    # Save the file\n",
        "    filepath.write_text(markdown_content, encoding='utf-8')\n",
        "    return filepath\n",
        "\n",
        "async def process_link(session, link, user_query, search_query):\n",
        "    \"\"\"\n",
        "    Process a single link: fetch the webpage, check if it's useful,\n",
        "    and if so, extract relevant context.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        page_text = await fetch_webpage_text_async(session, link)\n",
        "        if page_text:\n",
        "            is_useful = await is_page_useful_async(session, user_query, page_text)\n",
        "            if is_useful == \"Yes\":\n",
        "                context = await extract_relevant_context_async(session, user_query, search_query, page_text)\n",
        "                if context:\n",
        "                    print(f\"Extracted context from {link} (first 200 chars): {context[:200]}\")\n",
        "                    return context\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing link {link}: {e}\")\n",
        "    return None\n",
        "\n",
        "async def process_batch_async(session, tasks, batch_size=3):\n",
        "    \"\"\"\n",
        "    Process a list of tasks in batches to avoid overwhelming the API.\n",
        "    Returns list of results in the same order as tasks.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for i in range(0, len(tasks), batch_size):\n",
        "        batch = tasks[i:i + batch_size]\n",
        "        batch_results = await asyncio.gather(*batch)\n",
        "        results.extend(batch_results)\n",
        "        # Add delay between batches\n",
        "        if i + batch_size < len(tasks):\n",
        "            await asyncio.sleep(RATE_LIMIT_DELAY * 2)\n",
        "    return results\n",
        "\n",
        "async def process_links_batch(session, links, user_query, unique_links):\n",
        "    \"\"\"\n",
        "    Process links in small batches to avoid rate limits.\n",
        "    \"\"\"\n",
        "    link_tasks = [\n",
        "        process_link(session, link, user_query, unique_links[link])\n",
        "        for link in links\n",
        "    ]\n",
        "    return await process_batch_async(session, link_tasks, batch_size=3)\n",
        "\n",
        "# ...existing code before async_main...\n",
        "async def generate_final_report_async(session, user_query, aggregated_contexts):\n",
        "    \"\"\"\n",
        "    Stub function: combine contexts into a final report.\n",
        "    \"\"\"\n",
        "    if aggregated_contexts:\n",
        "        return \"Final Report:\\n\" + \"\\n\\n\".join(aggregated_contexts)\n",
        "    return \"No research findings.\"\n",
        "\n",
        "# =========================\n",
        "# Main Asynchronous Routine\n",
        "# =========================\n",
        "\n",
        "async def async_main():\n",
        "    user_query = input(\"Enter your research query/topic: \").strip()\n",
        "    iter_limit_input = input(\"Enter maximum number of iterations (default 10): \").strip()\n",
        "    iteration_limit = int(iter_limit_input) if iter_limit_input.isdigit() else 10\n",
        "\n",
        "    aggregated_contexts = []    # All useful contexts from every iteration\n",
        "    all_search_queries = []     # Every search query used across iterations\n",
        "    iteration = 0\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        # ----- INITIAL SEARCH QUERIES -----\n",
        "        new_search_queries = await generate_search_queries_async(session, user_query)\n",
        "        if not new_search_queries:\n",
        "            print(\"No search queries were generated by the LLM. Exiting.\")\n",
        "            return\n",
        "        all_search_queries.extend(new_search_queries)\n",
        "\n",
        "        # ----- ITERATIVE RESEARCH LOOP -----\n",
        "        while iteration < iteration_limit:\n",
        "            print(f\"\\n=== Iteration {iteration + 1} ===\")\n",
        "            iteration_contexts = []\n",
        "\n",
        "            # For each search query, perform SERPAPI searches concurrently.\n",
        "            search_tasks = [perform_search_async(session, query) for query in new_search_queries]\n",
        "            search_results = await asyncio.gather(*search_tasks)\n",
        "\n",
        "            # Aggregate all unique links from all search queries of this iteration.\n",
        "            # Map each unique link to the search query that produced it.\n",
        "            unique_links = {}\n",
        "            for idx, links in enumerate(search_results):\n",
        "                query = new_search_queries[idx]\n",
        "                for link in links:\n",
        "                    if link not in unique_links:\n",
        "                        unique_links[link] = query\n",
        "\n",
        "            print(f\"Aggregated {len(unique_links)} unique links from this iteration.\")\n",
        "\n",
        "            # Process links in batches\n",
        "            all_links = list(unique_links.keys())\n",
        "            link_results = []\n",
        "            for i in range(0, len(all_links), 3):\n",
        "                batch = all_links[i:i+3]\n",
        "                results = await process_links_batch(session, batch, user_query, unique_links)\n",
        "                link_results.extend(results)\n",
        "\n",
        "            # Collect non-None contexts.\n",
        "            for res in link_results:\n",
        "                if res:\n",
        "                    iteration_contexts.append(res)\n",
        "\n",
        "            if iteration_contexts:\n",
        "                aggregated_contexts.extend(iteration_contexts)\n",
        "            else:\n",
        "                print(\"No useful contexts were found in this iteration.\")\n",
        "\n",
        "            # ----- ASK THE LLM IF MORE SEARCHES ARE NEEDED -----\n",
        "            new_search_queries = await get_new_search_queries_async(session, user_query, all_search_queries, aggregated_contexts)\n",
        "            if new_search_queries == \"<done>\":\n",
        "                print(\"LLM indicated that no further research is needed.\")\n",
        "                break\n",
        "            elif new_search_queries:\n",
        "                print(\"LLM provided new search queries:\", new_search_queries)\n",
        "                all_search_queries.extend(new_search_queries)\n",
        "            else:\n",
        "                print(\"LLM did not provide any new search queries. Ending the loop.\")\n",
        "                break\n",
        "            iteration += 1\n",
        "\n",
        "        # ----- FINAL REPORT -----\n",
        "        print(\"\\nGenerating final report...\")\n",
        "        max_report_retries = 3\n",
        "        final_report = None\n",
        "\n",
        "        for retry in range(max_report_retries):\n",
        "            try:\n",
        "                final_report = await generate_final_report_async(session, user_query, aggregated_contexts)\n",
        "                if final_report:\n",
        "                    break\n",
        "                print(f\"Empty report received, retrying ({retry + 1}/{max_report_retries})...\")\n",
        "                await asyncio.sleep(RATE_LIMIT_DELAY * 2)\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating report (attempt {retry + 1}): {e}\")\n",
        "                await asyncio.sleep(RATE_LIMIT_DELAY * 2)\n",
        "\n",
        "        print(\"\\n==== FINAL REPORT ====\\n\")\n",
        "        if final_report:\n",
        "            print(final_report)\n",
        "            # Generate a report title and export report to a markdown file\n",
        "            report_title = await generate_report_title(session, user_query, aggregated_contexts)\n",
        "            file_path = save_report_as_markdown(report_title, final_report, user_query)\n",
        "            print(f\"Final report exported to: {file_path}\")\n",
        "        else:\n",
        "            print(\"Failed to generate report after all retries.\")\n",
        "            print(\"\\nCollected Contexts:\")\n",
        "            for idx, context in enumerate(aggregated_contexts, 1):\n",
        "                print(f\"\\nContext {idx}:\")\n",
        "                print(context[:500] + \"...\" if len(context) > 500 else context)\n",
        "\n",
        "def main():\n",
        "    # Instead of asyncio.run(), use get_event_loop() since we're in a notebook\n",
        "    loop = asyncio.get_event_loop()\n",
        "    loop.run_until_complete(async_main())\n",
        "\n",
        "# In a Jupyter notebook environment, just call main() directly\n",
        "# Remove the if __name__ == \"__main__\" check as it's not needed in notebooks\n",
        "main()\n"
      ]
    }
  ],
  "metadata": {
    "authorship_tag": "ABX9TyOe5BsaH0aplNCjknkFtnjg",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "provenance": []
  },
  "nbformat": 4,
  "nbformat_minor": 0
}